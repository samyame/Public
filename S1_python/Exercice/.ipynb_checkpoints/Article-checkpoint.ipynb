{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of_4.html\n",
    "\n",
    "https://towardsdatascience.com/principal-components-analysis-pca-fundamentals-benefits-insights-for-industry-2f03ad18c4d7\n",
    "\n",
    "https://medium.com/@dmitriy.kavyazin/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2\n",
    "\n",
    "https://www.qualtrics.com/experience-management/research/cluster-analysis/\n",
    "\n",
    "https://www.youtube.com/watch?v=FgakZw6K1QQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components & Custering Analysis Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Princicipal components and clustering analysis are two very useful and prominent technics in machine learning. Those are different technics that do not serve the same purpose. However, they are precious tools for data scientist like us to build and optimize their models and analysis. Hence, we will review their mutual benefits and the scenarios where they might be a good fit of complementarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a definition:\n",
    "\n",
    "\"Principal Component Analysis (PCA) is a statistical techniques used to reduce the dimensionality of the data (reduce the number of features in the dataset) by selecting the most important features that capture maximum information about the dataset\"\n",
    "\n",
    "In other words, PCA uses the correlation between variables of a dataset to produce new variables called \"dimensions\" that reduce the size of the data on the constraint of minimizing the loss of information.\n",
    "\n",
    "The output of the PCA are components which are a particular linear relation between our variables:\n",
    "The first component explains the highest degree of the variance and it goes down from here. The objective is two have the highest percentage of variance explaines by the smallest number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Ressources Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most intuitive benefit of the technic is the reduction of computation time and power. Indeed, by reducing the amount of data while keeping the information, the model can run faster with results relatively close to the original dataset.\n",
    "Instead of computing every piece of data, some chunks being useless or redundant, the model can focus more on the high-variance-explaining material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of dimensions brings many mechnical effects:\n",
    "\n",
    "    - The issue of overfitting arises when a model is linked too closely with a particular dataset. By reducing the redundancy and least informative part of the dataset, we can move away from the particular case and get closer to the underlying general rule of the data. The model thus focusing on the most informative dimensions without being distracted by every particularities of the data. It mechanically decreases the noise.\n",
    "    - The method also brings more representative samples, for the same reasons explicited before, which can be very useful in the case of bootstrapping.\n",
    "    - If you want to visualize the whole dataset at once, it will be difficult or impossible if there are more than 3 variables. The alternative is to visualize through a number of 2-D graphs but it will get difficult for our brains to connect those different graphs together. The PCA allows in this case to reduce dimensions and to visualize the relations between observations over one graph for more than 2 or 3 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
